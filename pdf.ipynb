{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### https://pymupdf.readthedocs.io/en/latest/recipes-annotations.html"
      ],
      "metadata": {
        "id": "JfdC14u9_ICA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "id": "a21hc4aqu2yS",
        "outputId": "e4033af2-1e27-45f6-8704-9d9041a150bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cQwSqPt4FSO",
        "outputId": "df14eb0e-f63d-4688-f2f9-91b7cecfb738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2545k  100 2545k    0     0   336k      0  0:00:07  0:00:07 --:--:--  610k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3161k  100 3161k    0     0   414k      0  0:00:07  0:00:07 --:--:--  938k\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!curl -o data/Singapore.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Singapore\n",
        "!curl -o data/Afghanistan.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Afghanistan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJemN4ao47Xl",
        "outputId": "ec779a06-82a9-4fad-bbe2-bd5f0c044727"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Collecting PyMuPDFb==1.24.9 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.9 pymupdf-1.24.9 pypdf-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader(\"data/Singapore.pdf\")\n",
        "number_of_pages = len(reader.pages)\n",
        "\n",
        "text = \"\"\n",
        "for page in range(number_of_pages):\n",
        "  page_text = reader.pages[page]\n",
        "  text += page_text.extract_text()"
      ],
      "metadata": {
        "id": "J-m9Scoc5xxV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page in range(0, number_of_pages-1):\n",
        "  page = reader.pages[page]\n",
        "  count = 0\n",
        "\n",
        "  for image_file_object in page.images:\n",
        "      with open(str(count) + image_file_object.name, \"wb\") as fp:\n",
        "          fp.write(image_file_object.data)\n",
        "          count += 1"
      ],
      "metadata": {
        "id": "8L-99SGE9dRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "MI5rsNY8a8HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Highlight text from pdf"
      ],
      "metadata": {
        "id": "iwO-CiK5odt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Regex': [r\"(?i)\\bworld\\b\", r\"\\$[0-9]+\", r\"\\((.*)\\)\"]}\n",
        "df = pd.DataFrame(data=data)\n",
        "\n",
        "def extract_sensitive_data(page_text, reg):\n",
        "    compiled = re.compile(reg)\n",
        "    return [word[:4] for word in page_text if compiled.search(word[4])]\n",
        "\n",
        "def redaction():\n",
        "  path = \"data\"\n",
        "  file_list = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
        "  for filename in file_list:\n",
        "    doc = fitz.open(os.path.join(path, filename ))\n",
        "\n",
        "    for page in doc:\n",
        "      for reg in df['Regex']:\n",
        "        sensitive_data = extract_sensitive_data(page.get_text(\"words\"), reg)\n",
        "        for area in sensitive_data:\n",
        "          annotation = page.add_highlight_annot(area)\n",
        "          annotation.set_colors(stroke=[0.5, 0.8, 0.8]) #Change colours RGB/255\n",
        "          annotation.update()\n",
        "\n",
        "      doc.save(f\"{os.path.splitext(filename)[0]}_edited.pdf\")\n",
        "\n",
        "redaction()"
      ],
      "metadata": {
        "id": "t8g60EdDAojd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pymupdf\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Regex': [r\"(?i)\\b[a-zA-Z]+istan\\b\"]}\n",
        "df = pd.DataFrame(data=data)\n",
        "\n",
        "def extract_sensitive_data(page_text, reg):\n",
        "    compiled = re.compile(reg)\n",
        "    return [word[:4] for word in page_text if compiled.search(word[4])]\n",
        "\n",
        "def redaction(df):\n",
        "  path = \"\"\n",
        "  file_list = ['Afghanistan_edited.pdf']\n",
        "  for filename in file_list:\n",
        "    doc = pymupdf.open(os.path.join(path, filename ))\n",
        "    print(\"Number of pages: \", doc.page_count)\n",
        "    for page in doc:\n",
        "      text = page.get_text(\"words\")\n",
        "      for phrase in df['Regex']:\n",
        "        sensitive_data = extract_sensitive_data(page.get_text(\"words\"), phrase)\n",
        "        for area in sensitive_data:\n",
        "          # page.add_underline_annot(area) Green underline is quite faint\n",
        "          # page.add_strikeout_annot(area) Red strikeout\n",
        "          # page.add_squiggly_annot(area) Purple underline squiggly\n",
        "          # page.add_highlight_annot(area) # Yellow highlight\n",
        "          # page.add_redact_annot(area, fill=(0, 0, 0), cross_out=False) Red box\n",
        "          # page.apply_redactions()\n",
        "    doc.save(f\"{os.path.splitext(filename)[0]}_edited.pdf\")\n",
        "\n",
        "\n",
        "redaction(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChXfxEA5tXJr",
        "outputId": "abb190e5-975a-4e58-d99c-bb1efdc68788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages:  80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipdeptree"
      ],
      "metadata": {
        "id": "K4OJRbb2OSnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e386bd-6b07-4603-83e6-0fcddca52c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pipdeptree in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from pipdeptree) (23.2)\n",
            "Requirement already satisfied: pip>=23.1.2 in /usr/local/lib/python3.10/dist-packages (from pipdeptree) (23.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarisation"
      ],
      "metadata": {
        "id": "TJID78-0Mwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz\n",
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "punctuation = punctuation + '\\n'\n",
        "\n",
        "def summariser(text):\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "\n",
        "  stopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "  words = word_tokenize(text)\n",
        "  freqTable = dict()\n",
        "  for word in words:\n",
        "      word = word.lower()\n",
        "      if word in stopWords:\n",
        "          continue\n",
        "      if word in punctuation:\n",
        "          continue\n",
        "      if word in freqTable:\n",
        "          freqTable[word] += 1\n",
        "      else:\n",
        "          freqTable[word] = 1\n",
        "\n",
        "  sentences = sent_tokenize(text)\n",
        "  sentenceValue = dict()\n",
        "\n",
        "  for sentence in sentences:\n",
        "      for word, freq in freqTable.items():\n",
        "          if word in sentence.lower():\n",
        "              if sentence in sentenceValue:\n",
        "                  sentenceValue[sentence] += freq\n",
        "              else:\n",
        "                  sentenceValue[sentence] = freq\n",
        "\n",
        "  sumValues = 0\n",
        "  for sentence in sentenceValue:\n",
        "      sumValues += sentenceValue[sentence]\n",
        "\n",
        "  average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "  summary = ''\n",
        "  for sentence in sentences:\n",
        "      if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "          summary += \" \" + sentence\n",
        "\n",
        "  final_summary = [sentence for sentence in sentences if (\n",
        "      sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average))]\n",
        "  summary = ' '.join(final_summary)\n",
        "\n",
        "  return final_summary\n",
        "\n",
        "def extract_sensitive_data(page_text, reg):\n",
        "    compiled = re.compile(reg)\n",
        "    #print(compiled)\n",
        "    return [word[:4] for word in page_text if compiled.search(word[4])]\n",
        "\n",
        "_special_chars_map = {i: '\\\\' + chr(i) for i in b'()[]{}?*+-|^$\\\\.&~#\\t\\n\\r\\v\\f'}\n",
        "\n",
        "def escape(pattern):\n",
        "    \"\"\"\n",
        "    Escape special characters in a string.\n",
        "    \"\"\"\n",
        "    if isinstance(pattern, str):\n",
        "        return pattern.translate(_special_chars_map)\n",
        "    else:\n",
        "        pattern = str(pattern, 'latin1')\n",
        "        return pattern.translate(_special_chars_map).encode('latin1')\n",
        "\n",
        "def redaction():\n",
        "  path = \"data\"\n",
        "  file_list = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
        "  for filename in file_list:\n",
        "    doc = fitz.open(os.path.join(path, filename ))\n",
        "    page_text = \"\"\n",
        "    for page in doc:\n",
        "      text = page.get_text(\"text\")\n",
        "      page_text += text\n",
        "\n",
        "    page_text = re.sub(r\"\\[[a-zA-Z0-9]+\\]\", \"\", page_text)\n",
        "\n",
        "    reg_list = [r\"(?i)\\b{}\\b\".format(escape(item)).replace('\\\\n','') for item in summariser(page_text)]\n",
        "    for reg in reg_list:\n",
        "        sensitive_data = extract_sensitive_data(page.get_text(\"words\"), reg)\n",
        "        print(reg)\n",
        "        for area in sensitive_data:\n",
        "          page.add_highlight_annot(area)\n",
        "\n",
        "    doc.save(f\"{filename}_edited.pdf\")\n",
        "    return\n",
        "text = redaction()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5MHyrYE1MnNl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz\n",
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "\n",
        "def redaction():\n",
        "  path = \"data\"\n",
        "  file_list = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
        "  for filename in file_list:\n",
        "    doc = fitz.open(os.path.join(path, filename ))\n",
        "    page_text = \"\"\n",
        "    for page in doc:\n",
        "      text = page.get_text(\"text\")\n",
        "      page_text += text\n",
        "    return page_text\n",
        "\n",
        "text = redaction()\n",
        "\n",
        "stopwords = list(STOP_WORDS)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "punctuation = punctuation + '\\n'\n",
        "\n",
        "\n",
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1\n",
        "\n",
        "max_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "\n",
        "\n",
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "\n",
        "select_length = int(len(sentence_tokens)*0.4)\n",
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "\n",
        "final_summary = [word.text for word in summary]\n",
        "summary = ' '.join(final_summary)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "sMZEWxdYiDxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER"
      ],
      "metadata": {
        "id": "Log8l0UTrm_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "QzjUFgXvutaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import random\n",
        "from spacy.tokens import Doc\n",
        "from spacy.training import Example\n",
        "import re\n",
        "\n",
        "regex = r\"(?i)\\b(op|operation|obj|objective)\\s+[A-Z]+\"\n",
        "\n",
        "text = \"\"\"\n",
        "On last Tuesday the operation EDGWARE was carried out. We followed 3 tangos to HENDON but they were lost at GOLDERS GREEN\n",
        "but we got rid of the idiot so then we found the enemy in Objective BARNET and followed them all the way where we engauged them in\n",
        "FINCHLEY and carried the bodies back to BANK.\n",
        "\"\"\"\n",
        "phrase_dict = {'EDGWARE': 'Op name', 'HIGH BARNET': 'Op name', 'HENDON': 'Op name', 'FINCHLEY': 'Op name', 'BRENT CROSS': 'Op name'}\n",
        "\n",
        "train_data = [(keys, [(0, len(keys), values)]) for keys, values in phrase_dict.items()]\n",
        "print(train_data)\n",
        "\n",
        "phrase_patterns = [nlp(text) for text in phrase_dict]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp.disable_pipes('tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer')\n",
        "print(\"   Training ...\")\n",
        "optimizer = nlp.create_optimizer()\n",
        "for _ in range(25):\n",
        "    random.shuffle(train_data)\n",
        "    for raw_text, entity_offsets in train_data:\n",
        "        doc = nlp.make_doc(raw_text)\n",
        "        example = Example.from_dict(doc, {\"entities\": entity_offsets})\n",
        "        nlp.update([example], sgd=optimizer)\n",
        "\n",
        "# Result after training\n",
        "print(f\"Result AFTER training:\")\n",
        "doc = nlp(text)\n",
        "for token in doc.ents:\n",
        "  print(token.text, token.label_, token.start_char, token.end_char)\n",
        "\n",
        "spacy.explain(\"GPE\")\n",
        "displacy.render(doc, style=\"ent\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "yHmRe-OHrrF4",
        "outputId": "61a7e77f-b7c2-41c9-f930-ba31667e77db"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('EDGWARE', [(0, 7, 'Op name')]), ('HIGH BARNET', [(0, 11, 'Op name')]), ('HENDON', [(0, 6, 'Op name')]), ('FINCHLEY', [(0, 8, 'Op name')]), ('BRENT CROSS', [(0, 11, 'Op name')])]\n",
            "   Training ...\n",
            "Result AFTER training:\n",
            "last Tuesday DATE 4 16\n",
            "EDGWARE Op name 31 38\n",
            "3 CARDINAL 68 69\n",
            "HENDON Op name 80 86\n",
            "GOLDERS Op name 109 116\n",
            "FINCHLEY Op name 256 264\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>On \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    last Tuesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " the operation \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    EDGWARE\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Op name</span>\n",
              "</mark>\n",
              " was carried out. We followed \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    3\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " tangos to \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    HENDON\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Op name</span>\n",
              "</mark>\n",
              " but they were lost at \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    GOLDERS\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Op name</span>\n",
              "</mark>\n",
              " GREEN <br>but we got rid of the idiot so then we found the enemy in Objective BARNET and followed them all the way where we engauged them in <br>\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    FINCHLEY\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Op name</span>\n",
              "</mark>\n",
              " and carried the bodies back to BANK.<br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh-qifdSAcMI",
        "outputId": "4fc5cbc5-1c5a-4c75-adb5-8e4208dceefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('robot', [(0, 5, 'TECHNOLOGY')]), ('economy', [(0, 7, 'MONEY')])]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_d9PmyuvvTI",
        "outputId": "afe22e9c-060b-4814-e77d-5ec40d15912b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "On last Tuesday the operation EDGWARE was carried out. We followed 3 tangos to HENDON but they were lost at GOLDERS GREEN\n",
        "but we got rid of the idiot so then we found the enemy in Objective BARNET and followed them all the way where we engauged them in\n",
        "FINCHLEY and carried the bodies back to BANK.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZGivqXHmc8kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'name': ['hendon', 'edgware', 'golders green', 'brent cross', 'high barnet'],\n",
        "                   'description': ['zone 3', 'zone 5', 'zone 3', 'zone 4', 'zone 5']})\n",
        "df['Op name'] = df['name']\n",
        "df=df.drop(columns=['name', 'description'])\n",
        "df.to_csv('my_file.csv', index=False)"
      ],
      "metadata": {
        "id": "RIYm3CrQiOXm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('my_file.csv') as f:\n",
        "    phrase_list = [line for line in csv.DictReader(f)]\n",
        "train_data = [\n",
        "    (values, [(0, len(values), keys)])\n",
        "    for phrase_dict in phrase_list\n",
        "    for keys, values in phrase_dict.items()\n",
        "]\n",
        "\n",
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWlMZddIdoNy",
        "outputId": "9fe30d79-a699-4c4b-a4be-da1301ceb5ce"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hendon', [(0, 6, 'Op name')]), ('edgware', [(0, 7, 'Op name')]), ('golders green', [(0, 13, 'Op name')]), ('brent cross', [(0, 11, 'Op name')]), ('high barnet', [(0, 11, 'Op name')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language\n",
        "@Language.component(\"remove\")\n",
        "def remove_ent(doc):\n",
        "    ents = list(doc.ents)\n",
        "    for ent in ents:\n",
        "        if ent.label_ in  [\"CARDINAL\", \"DATE\"]:\n",
        "            ents.remove(ent)\n",
        "    ents = tuple(ents)\n",
        "    doc.ents = ents\n",
        "    return (doc)\n",
        "Language.component(\"remove\", func=remove_ent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "vs00utS8HDhD",
        "outputId": "d20d1354-7705-43ec-ba16-40f9328c6db7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.remove_ent(doc)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>remove_ent</b><br/>def remove_ent(doc)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-69-8d0cb11f093f&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "from spacy import util\n",
        "from spacy.tokens import Doc\n",
        "from spacy.training import Example\n",
        "from spacy.language import Language\n",
        "\n",
        "def print_doc_entities(_doc: Doc):\n",
        "    if _doc.ents:\n",
        "        for _ent in _doc.ents:\n",
        "            print(f\"     {_ent.text} {_ent.label_} {_ent.start_char} {_ent.end_char}\")\n",
        "    else:\n",
        "        print(\"     NONE\")\n",
        "\n",
        "def customizing_pipeline_component(nlp: Language):\n",
        "    phrase_dict = {'EDGWARE': 'Op name', 'HIGH BARNET': 'Op name', 'HENDON': 'Op name',\n",
        "                   'FINCHLEY': 'Op name', 'BRENT CROSS': 'Op name'}\n",
        "\n",
        "    train_data = [(keys, [(0, len(keys), values)]) for keys, values in phrase_dict.items()]\n",
        "\n",
        "    # Result before training\n",
        "    print(f\"\\nResult BEFORE training:\")\n",
        "    doc = nlp(text)\n",
        "    print_doc_entities(doc)\n",
        "\n",
        "    # Disable all pipe components except 'ner'\n",
        "    disabled_pipes = []\n",
        "    for pipe_name in nlp.pipe_names:\n",
        "        if pipe_name != 'ner':\n",
        "            nlp.disable_pipes(pipe_name)\n",
        "            disabled_pipes.append(pipe_name)\n",
        "\n",
        "    print(\"   Training ...\")\n",
        "    optimizer = nlp.create_optimizer()\n",
        "    for _ in range(25):\n",
        "        random.shuffle(train_data)\n",
        "        for raw_text, entity_offsets in train_data:\n",
        "            doc = nlp.make_doc(raw_text)\n",
        "            example = Example.from_dict(doc, {\"entities\": entity_offsets})\n",
        "            nlp.update([example], sgd=optimizer)\n",
        "\n",
        "    # Enable all previously disabled pipe components\n",
        "    for pipe_name in disabled_pipes:\n",
        "        nlp.enable_pipe(pipe_name)\n",
        "\n",
        "    # Result after training\n",
        "    print(f\"Result AFTER training:\")\n",
        "    doc = nlp(text)\n",
        "    print_doc_entities(doc)\n",
        "\n",
        "def main():\n",
        "    nlp = spacy.load('en_core_web_md')\n",
        "    nlp.add_pipe(\"remove\")\n",
        "    customizing_pipeline_component(nlp)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "FhBnNSus7E7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675c5523-07d5-4a4d-b487-3425594124ea"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result BEFORE training:\n",
            "     EDGWARE PERSON 31 38\n",
            "     HENDON LOC 80 86\n",
            "     GOLDERS ORG 109 116\n",
            "   Training ...\n",
            "Result AFTER training:\n",
            "     EDGWARE Op name 31 38\n",
            "     HENDON Op name 80 86\n",
            "     GOLDERS Op name 109 116\n",
            "     FINCHLEY Op name 256 264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### PHRASE MATCHER\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
        "# Only run nlp.make_doc to speed things up\n",
        "patterns = [nlp.make_doc(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", patterns)\n",
        "\n",
        "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
        "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSz_4suRDjcu",
        "outputId": "3423e821-6bc3-457e-dc2e-30b67d1ac329"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Angela Merkel\n",
            "Barack Obama\n",
            "Washington, D.C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I may have a solution that involves a few steps. I haven't fully tested it and it may still have some bugs. One problem is that it takes a long time if the PDF is large and the character range is at the end of the PDF.\n",
        "\n",
        "First, I try to find the blocks that contain the character range and combine them into a block rectangle. Since it might be on more than one page, it is split into parts for each page.\n",
        "Second, I use this range as a clip parameter to find the words and iterate through the words to find the rectangles that contain parts of the term.\n",
        "Finally, I combine the rectangles found that are on the same line. So in the end I have single rectangles per page and line that contain the range of characters.\n"
      ],
      "metadata": {
        "id": "Dbx_fBg8Cght"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "\n",
        "def find_blocks(pdf_file: str, start_char: int, end_char: int):\n",
        "    \"\"\"\n",
        "    Find the blocks containing the character range. Return the block rectangle with the corresponding page number.\n",
        "        :param pdf_file: Path to the PDF file.\n",
        "        :param start_char: Start character of the range.\n",
        "        :param end_char: End character of the range.\n",
        "    \"\"\"\n",
        "    dict_blocks = {}\n",
        "    block_text = \"\"\n",
        "    with fitz.open(pdf_file) as doc:\n",
        "        for page in doc:\n",
        "            blocks = page.get_text(\"blocks\")\n",
        "            for block in blocks:\n",
        "                block_text += block[4]\n",
        "                if len(block_text) > start_char:\n",
        "                    if page.number in dict_blocks:\n",
        "                        dict_blocks[page.number].append(block[:4])\n",
        "                    else:\n",
        "                        dict_blocks[page.number] = [block[:4]]\n",
        "                if len(block_text) > end_char:\n",
        "                    return dict_blocks\n",
        "\n",
        "def find_text_coordinates_on_page(pdf_file: str, page_number: int, block_box: fitz.Rect, term: str):\n",
        "    \"\"\"\n",
        "    Find the exact coordinates of the text given by the character range.\n",
        "        :param pdf_file: Path to the PDF file.\n",
        "        :param page_number: Page number of the PDF file.\n",
        "        :param block_box: The block rectangle containing the character range.\n",
        "        :param term: The text to find the coordinates for.\n",
        "    \"\"\"\n",
        "    with fitz.open(pdf_file) as doc:\n",
        "        page = doc[page_number]\n",
        "        words = page.get_text(\"words\", clip=block_box)\n",
        "        term = term.replace(\"\\n\", \" \")\n",
        "        term_list = term.split(\" \")\n",
        "        term_list = [term for term in term_list if term]\n",
        "        text_coordinates = []\n",
        "        if not term_list:\n",
        "            return text_coordinates\n",
        "        # iterate through the words and find the coordinates of the term\n",
        "        for word in words:\n",
        "            if term_list[0] in word[4]:  # check if the first word in the term is in the word\n",
        "                term_found = True\n",
        "                text_coordinates.append(fitz.Rect(word[:4]))\n",
        "                if len(term_list) == 1:  # if the term has only one word, return the coordinates\n",
        "                    return text_coordinates\n",
        "                # check if the next words in the term are in the next words\n",
        "                index = words.index(word)\n",
        "                for i in range(1, len(term_list)):\n",
        "                    if index + i >= len(words):  # if the index is out of range, break the loop\n",
        "                        term_found = False\n",
        "                        break\n",
        "                    text_coordinates.append(fitz.Rect(words[index + i][:4]))\n",
        "                    if term_list[i] not in words[index + i][4]:  # if the word is not in the next word, break the loop\n",
        "                        text_coordinates = []\n",
        "                        term_found = False\n",
        "                        break\n",
        "                if term_found:  # if the term is found, return the coordinates\n",
        "                    return text_coordinates\n",
        "        return text_coordinates\n",
        "\n",
        "\n",
        "def combine_rects(rects: list):\n",
        "    \"\"\"\n",
        "    Function to combine multiple rectangles into one.\n",
        "        :param rects: List of rectangles to combine.\n",
        "    \"\"\"\n",
        "    x0 = min(rect[0] for rect in rects)\n",
        "    y0 = min(rect[1] for rect in rects)\n",
        "    x1 = max(rect[2] for rect in rects)\n",
        "    y1 = max(rect[3] for rect in rects)\n",
        "    return fitz.Rect(x0, y0, x1, y1)\n",
        "\n",
        "\n",
        "def is_same_line(rect1: fitz.Rect, rect2: fitz.Rect, tolerance=5):\n",
        "    \"\"\"Check if two rectangles are on the same line.\n",
        "        :param rect1: the first rectangle\n",
        "        :param rect2: the second rectangle\n",
        "        :param tolerance: the tolerance value\n",
        "        :return: True if the rectangles are on the same line, False otherwise\"\"\"\n",
        "    # Check if two rectangles are on the same line with a tolerance value\n",
        "    return abs(rect1.y0 - rect2.y0) < tolerance  # return True if the rectangles are on the same line\n",
        "\n",
        "\n",
        "def combine_rectangles_on_same_line(rectangles: list,):\n",
        "    \"\"\"Combine rectangles on the same line.\n",
        "        :param rectangles: the rectangles to combine\n",
        "        :return: the combined rectangles\"\"\"\n",
        "    result = []  # list of combined rectangles\n",
        "    grouped_rectangles = []  # list of grouped rectangles\n",
        "    print(rectangles)\n",
        "    rectangles.sort(key=lambda rect: rect.y0)  # sort rectangles by their y-coordinate\n",
        "\n",
        "    for rect in rectangles:  # iterate over all rectangles\n",
        "        # If grouped_rectangles is empty or the current rect is on the same line and does not contain the same word\n",
        "        if not grouped_rectangles or (is_same_line(grouped_rectangles[-1], rect)):\n",
        "            grouped_rectangles.append(rect)\n",
        "        else:\n",
        "            # Combine rectangles on the same line with different text\n",
        "            combined_rect = combine_rects(grouped_rectangles)\n",
        "            result.append(combined_rect)\n",
        "            grouped_rectangles = [rect]\n",
        "\n",
        "    # Add the last group of rectangles\n",
        "    if grouped_rectangles:\n",
        "        combined_rect = combine_rects(grouped_rectangles)\n",
        "        result.append(combined_rect)\n",
        "\n",
        "    return result  # return the combined rectangles\n",
        "\n",
        "def combine_functions(start_char: int, end_char: int, pdf_path: str):\n",
        "    \"\"\"\n",
        "    Combine the functions to find the coordinates for a given character range.\n",
        "        :param start_char: Start character of the range.\n",
        "        :param end_char: End character of the range.\n",
        "        :param pdf_path: Path to the PDF file.\n",
        "        :return: List of combined rectangles.\n",
        "    \"\"\"\n",
        "    # Open the PDF file and extract the text\n",
        "    pdf_text = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            pdf_text += page.get_text()\n",
        "    term = pdf_text[start_char:end_char]\n",
        "    # Find the blocks containing the character range for each page (term might be on multiple pages)\n",
        "    dict_blocks = find_blocks(pdf_path, start_char, end_char)\n",
        "    combined_rects = []\n",
        "    # Find the text coordinates for the term (for each part of the term on the page) and combine them to one rectangle\n",
        "    for page_number, span_boxes in dict_blocks.items():\n",
        "        combined_block_boxes = combine_rects(span_boxes)  # combine the block boxes on the same page\n",
        "        # find the text coordinates for the part of the term that on the page\n",
        "        text_coordinates = find_text_coordinates_on_page(pdf_path, page_number, combined_block_boxes, term)\n",
        "        if not text_coordinates:\n",
        "            continue\n",
        "        combined_rect_on_line = combine_rectangles_on_same_line(text_coordinates)\n",
        "        combined_rects.append(combined_rect_on_line)\n",
        "\n",
        "    return combined_rects\n",
        "\n",
        "path = \"path/to/file\"\n",
        "start_char = 52\n",
        "end_char = 110\n",
        "combined_rects = combine_functions(start_char, end_char, path)`"
      ],
      "metadata": {
        "id": "o30uDaTVCX2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy pipeline"
      ],
      "metadata": {
        "id": "dDQlZ4u9LToN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vMC3jfaXLykR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(text)\n",
        "\n",
        "phrase_dict = {'robot': 'TECHNOLOGY', 'economy': 'MONEY'}\n",
        "\n",
        "train_data = [(keys, [(0, len(keys), values)]) for keys, values in phrase_dict.items()]"
      ],
      "metadata": {
        "id": "XZUgHLO2MxoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}